{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Many-to-few RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will construct a many-to-one RNN how how they many be used for classification and prediction\n",
    "ecurrent neural network is any network with nodes that update their state between *prediction* runs. This is in contrast to a perceptron or CNN which only updates it state during training. A RNN is composted of __memory cells__, which hold a state between prediction runs, the most popular of which are LSTM and GRU cells. RNN's are trained by \"unfolding\" them to the desired input size, with the weights of each cell shared across the unfolding:\n",
    "\n",
    "<img width=800px src=\"https://raw.githubusercontent.com/tipthederiver/Math-7243-2020/master/Labs/Lab%206/Lab%206%20RNN%20Unroll.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " We will Construct a __many to few__ RNN for the classification of text sentiment. We will be classifying the IMDB user comments database to see to see if reviews are positive or negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data['review']       # Reviews/Input\n",
    "y_data = data['sentiment']    # Sentiment/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the reviews:\n",
    "As reviews are unclean in the original dataset, we will clean them by removing html tags, non-alphabet such as punctuation and numbers and stop words. After that we will also lower_case them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.replace({'<.*?>': ''}, regex = True)    # remove html tag\n",
    "x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "x_data = x_data.apply(lambda review: [w for w in review.split() if w not in stopwords ])  # remove stop words\n",
    "x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Sentiments\n",
    "We will encode are sentiments such that 1 means positive and 0 means negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data.replace('positive', 1)\n",
    "y_data = y_data.replace('negative', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Splitting the data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "2672     [in, hoot, logan, lerman, plays, roy, eberhard...\n",
      "27263    [the, producers, made, big, mistake, casting, ...\n",
      "7401     [ahh, dull, v, shows, pilots, slammed, togethe...\n",
      "39408    [this, movie, commits, i, would, call, emotion...\n",
      "7752     [this, movie, stupid, i, want, back, i, paid, ...\n",
      "                               ...                        \n",
      "49450    [i, sort, accidentally, ended, watching, movie...\n",
      "4537     [this, movie, so, stupid, i, could, bare, watc...\n",
      "2760     [watching, midnight, cowboy, like, taking, mas...\n",
      "27525    [everybody, wants, editor, watch, movie, it, s...\n",
      "32705    [this, movie, visually, stunning, who, cares, ...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "35907    [just, fellow, movie, fans, get, point, film, ...\n",
      "25313    [yumiko, wakana, sakai, pretty, adopted, daugh...\n",
      "30861    [the, dominating, conflict, couple, fine, acto...\n",
      "24752    [simply, great, movie, doubt, great, story, su...\n",
      "4978     [clich, ridden, story, impending, divorce, eye...\n",
      "                               ...                        \n",
      "16168    [i, mean, sound, pretentious, call, next, cult...\n",
      "25950    [the, first, half, movie, quite, good, it, int...\n",
      "37133    [if, folks, really, stupid, i, could, srw, sup...\n",
      "36052    [i, understand, underrated, imdb, this, movie,...\n",
      "11122    [by, numbers, oscar, hungry, biopic, late, gre...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "2672     1\n",
      "27263    0\n",
      "7401     0\n",
      "39408    0\n",
      "7752     0\n",
      "        ..\n",
      "49450    0\n",
      "4537     0\n",
      "2760     1\n",
      "27525    0\n",
      "32705    1\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "35907    0\n",
      "25313    0\n",
      "30861    0\n",
      "24752    1\n",
      "4978     0\n",
      "        ..\n",
      "16168    0\n",
      "25950    0\n",
      "37133    1\n",
      "36052    1\n",
      "11122    0\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating max length of a review\n",
    "max_length=[]\n",
    "for review in x_train:\n",
    "    max_length.append(len(review))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[103,\n",
       " 75,\n",
       " 168,\n",
       " 90,\n",
       " 98,\n",
       " 127,\n",
       " 277,\n",
       " 140,\n",
       " 200,\n",
       " 246,\n",
       " 90,\n",
       " 106,\n",
       " 195,\n",
       " 65,\n",
       " 226,\n",
       " 180,\n",
       " 59,\n",
       " 135,\n",
       " 132,\n",
       " 83,\n",
       " 134,\n",
       " 143,\n",
       " 87,\n",
       " 80,\n",
       " 59,\n",
       " 114,\n",
       " 80,\n",
       " 42,\n",
       " 80,\n",
       " 108,\n",
       " 46,\n",
       " 115,\n",
       " 574,\n",
       " 75,\n",
       " 104,\n",
       " 25,\n",
       " 97,\n",
       " 80,\n",
       " 86,\n",
       " 98,\n",
       " 184,\n",
       " 82,\n",
       " 64,\n",
       " 103,\n",
       " 135,\n",
       " 85,\n",
       " 106,\n",
       " 77,\n",
       " 466,\n",
       " 424,\n",
       " 76,\n",
       " 103,\n",
       " 95,\n",
       " 74,\n",
       " 81,\n",
       " 136,\n",
       " 37,\n",
       " 55,\n",
       " 76,\n",
       " 48,\n",
       " 70,\n",
       " 60,\n",
       " 83,\n",
       " 120,\n",
       " 89,\n",
       " 87,\n",
       " 386,\n",
       " 116,\n",
       " 125,\n",
       " 96,\n",
       " 79,\n",
       " 80,\n",
       " 319,\n",
       " 83,\n",
       " 101,\n",
       " 169,\n",
       " 71,\n",
       " 89,\n",
       " 196,\n",
       " 153,\n",
       " 20,\n",
       " 65,\n",
       " 107,\n",
       " 76,\n",
       " 76,\n",
       " 39,\n",
       " 91,\n",
       " 76,\n",
       " 68,\n",
       " 203,\n",
       " 262,\n",
       " 105,\n",
       " 72,\n",
       " 248,\n",
       " 160,\n",
       " 166,\n",
       " 152,\n",
       " 86,\n",
       " 69,\n",
       " 67,\n",
       " 170,\n",
       " 83,\n",
       " 102,\n",
       " 82,\n",
       " 136,\n",
       " 79,\n",
       " 46,\n",
       " 204,\n",
       " 473,\n",
       " 133,\n",
       " 86,\n",
       " 55,\n",
       " 138,\n",
       " 138,\n",
       " 67,\n",
       " 73,\n",
       " 167,\n",
       " 211,\n",
       " 72,\n",
       " 195,\n",
       " 49,\n",
       " 290,\n",
       " 131,\n",
       " 170,\n",
       " 192,\n",
       " 77,\n",
       " 205,\n",
       " 153,\n",
       " 104,\n",
       " 172,\n",
       " 430,\n",
       " 92,\n",
       " 108,\n",
       " 138,\n",
       " 63,\n",
       " 127,\n",
       " 384,\n",
       " 113,\n",
       " 92,\n",
       " 153,\n",
       " 37,\n",
       " 114,\n",
       " 339,\n",
       " 433,\n",
       " 86,\n",
       " 100,\n",
       " 59,\n",
       " 86,\n",
       " 72,\n",
       " 184,\n",
       " 52,\n",
       " 83,\n",
       " 98,\n",
       " 149,\n",
       " 68,\n",
       " 115,\n",
       " 183,\n",
       " 69,\n",
       " 108,\n",
       " 64,\n",
       " 86,\n",
       " 66,\n",
       " 174,\n",
       " 99,\n",
       " 125,\n",
       " 110,\n",
       " 79,\n",
       " 90,\n",
       " 92,\n",
       " 208,\n",
       " 127,\n",
       " 67,\n",
       " 130,\n",
       " 82,\n",
       " 334,\n",
       " 24,\n",
       " 29,\n",
       " 287,\n",
       " 89,\n",
       " 32,\n",
       " 186,\n",
       " 63,\n",
       " 164,\n",
       " 68,\n",
       " 178,\n",
       " 236,\n",
       " 59,\n",
       " 80,\n",
       " 54,\n",
       " 77,\n",
       " 46,\n",
       " 204,\n",
       " 107,\n",
       " 181,\n",
       " 185,\n",
       " 72,\n",
       " 68,\n",
       " 332,\n",
       " 94,\n",
       " 40,\n",
       " 54,\n",
       " 97,\n",
       " 165,\n",
       " 99,\n",
       " 238,\n",
       " 27,\n",
       " 118,\n",
       " 73,\n",
       " 91,\n",
       " 66,\n",
       " 289,\n",
       " 168,\n",
       " 66,\n",
       " 46,\n",
       " 91,\n",
       " 71,\n",
       " 44,\n",
       " 141,\n",
       " 69,\n",
       " 182,\n",
       " 69,\n",
       " 93,\n",
       " 60,\n",
       " 67,\n",
       " 240,\n",
       " 435,\n",
       " 62,\n",
       " 69,\n",
       " 72,\n",
       " 62,\n",
       " 103,\n",
       " 82,\n",
       " 18,\n",
       " 222,\n",
       " 36,\n",
       " 336,\n",
       " 175,\n",
       " 76,\n",
       " 124,\n",
       " 107,\n",
       " 75,\n",
       " 54,\n",
       " 26,\n",
       " 58,\n",
       " 164,\n",
       " 74,\n",
       " 85,\n",
       " 157,\n",
       " 82,\n",
       " 57,\n",
       " 70,\n",
       " 82,\n",
       " 88,\n",
       " 109,\n",
       " 39,\n",
       " 127,\n",
       " 65,\n",
       " 256,\n",
       " 116,\n",
       " 58,\n",
       " 79,\n",
       " 108,\n",
       " 48,\n",
       " 183,\n",
       " 68,\n",
       " 285,\n",
       " 147,\n",
       " 171,\n",
       " 76,\n",
       " 393,\n",
       " 46,\n",
       " 31,\n",
       " 11,\n",
       " 102,\n",
       " 146,\n",
       " 103,\n",
       " 165,\n",
       " 287,\n",
       " 146,\n",
       " 69,\n",
       " 315,\n",
       " 85,\n",
       " 59,\n",
       " 25,\n",
       " 97,\n",
       " 172,\n",
       " 264,\n",
       " 114,\n",
       " 108,\n",
       " 99,\n",
       " 80,\n",
       " 409,\n",
       " 28,\n",
       " 149,\n",
       " 113,\n",
       " 218,\n",
       " 101,\n",
       " 143,\n",
       " 442,\n",
       " 67,\n",
       " 90,\n",
       " 245,\n",
       " 418,\n",
       " 68,\n",
       " 163,\n",
       " 206,\n",
       " 121,\n",
       " 42,\n",
       " 103,\n",
       " 35,\n",
       " 242,\n",
       " 90,\n",
       " 160,\n",
       " 59,\n",
       " 363,\n",
       " 85,\n",
       " 268,\n",
       " 85,\n",
       " 51,\n",
       " 29,\n",
       " 67,\n",
       " 568,\n",
       " 228,\n",
       " 195,\n",
       " 24,\n",
       " 212,\n",
       " 17,\n",
       " 63,\n",
       " 301,\n",
       " 82,\n",
       " 91,\n",
       " 81,\n",
       " 104,\n",
       " 90,\n",
       " 107,\n",
       " 146,\n",
       " 204,\n",
       " 23,\n",
       " 439,\n",
       " 57,\n",
       " 62,\n",
       " 123,\n",
       " 71,\n",
       " 131,\n",
       " 313,\n",
       " 195,\n",
       " 23,\n",
       " 67,\n",
       " 59,\n",
       " 90,\n",
       " 24,\n",
       " 58,\n",
       " 359,\n",
       " 104,\n",
       " 151,\n",
       " 206,\n",
       " 144,\n",
       " 70,\n",
       " 229,\n",
       " 69,\n",
       " 101,\n",
       " 38,\n",
       " 240,\n",
       " 66,\n",
       " 72,\n",
       " 90,\n",
       " 75,\n",
       " 127,\n",
       " 133,\n",
       " 87,\n",
       " 88,\n",
       " 163,\n",
       " 86,\n",
       " 64,\n",
       " 415,\n",
       " 153,\n",
       " 49,\n",
       " 23,\n",
       " 63,\n",
       " 50,\n",
       " 56,\n",
       " 75,\n",
       " 127,\n",
       " 81,\n",
       " 203,\n",
       " 94,\n",
       " 242,\n",
       " 331,\n",
       " 63,\n",
       " 309,\n",
       " 186,\n",
       " 109,\n",
       " 68,\n",
       " 64,\n",
       " 157,\n",
       " 73,\n",
       " 71,\n",
       " 94,\n",
       " 87,\n",
       " 93,\n",
       " 84,\n",
       " 70,\n",
       " 128,\n",
       " 229,\n",
       " 80,\n",
       " 270,\n",
       " 124,\n",
       " 88,\n",
       " 66,\n",
       " 465,\n",
       " 92,\n",
       " 72,\n",
       " 97,\n",
       " 100,\n",
       " 89,\n",
       " 32,\n",
       " 87,\n",
       " 86,\n",
       " 81,\n",
       " 217,\n",
       " 81,\n",
       " 272,\n",
       " 112,\n",
       " 77,\n",
       " 165,\n",
       " 44,\n",
       " 26,\n",
       " 66,\n",
       " 76,\n",
       " 153,\n",
       " 101,\n",
       " 77,\n",
       " 109,\n",
       " 56,\n",
       " 94,\n",
       " 61,\n",
       " 36,\n",
       " 63,\n",
       " 149,\n",
       " 179,\n",
       " 78,\n",
       " 42,\n",
       " 49,\n",
       " 68,\n",
       " 81,\n",
       " 254,\n",
       " 311,\n",
       " 230,\n",
       " 149,\n",
       " 67,\n",
       " 103,\n",
       " 107,\n",
       " 125,\n",
       " 89,\n",
       " 68,\n",
       " 86,\n",
       " 62,\n",
       " 72,\n",
       " 59,\n",
       " 334,\n",
       " 201,\n",
       " 145,\n",
       " 76,\n",
       " 71,\n",
       " 149,\n",
       " 66,\n",
       " 135,\n",
       " 165,\n",
       " 88,\n",
       " 167,\n",
       " 453,\n",
       " 88,\n",
       " 38,\n",
       " 79,\n",
       " 27,\n",
       " 69,\n",
       " 241,\n",
       " 193,\n",
       " 67,\n",
       " 87,\n",
       " 95,\n",
       " 104,\n",
       " 180,\n",
       " 96,\n",
       " 76,\n",
       " 82,\n",
       " 328,\n",
       " 373,\n",
       " 260,\n",
       " 106,\n",
       " 87,\n",
       " 83,\n",
       " 54,\n",
       " 142,\n",
       " 496,\n",
       " 140,\n",
       " 101,\n",
       " 44,\n",
       " 36,\n",
       " 50,\n",
       " 135,\n",
       " 110,\n",
       " 133,\n",
       " 208,\n",
       " 42,\n",
       " 94,\n",
       " 95,\n",
       " 89,\n",
       " 74,\n",
       " 56,\n",
       " 85,\n",
       " 212,\n",
       " 60,\n",
       " 14,\n",
       " 164,\n",
       " 104,\n",
       " 88,\n",
       " 92,\n",
       " 53,\n",
       " 85,\n",
       " 106,\n",
       " 78,\n",
       " 280,\n",
       " 61,\n",
       " 64,\n",
       " 128,\n",
       " 104,\n",
       " 101,\n",
       " 28,\n",
       " 37,\n",
       " 100,\n",
       " 94,\n",
       " 94,\n",
       " 157,\n",
       " 101,\n",
       " 82,\n",
       " 36,\n",
       " 101,\n",
       " 178,\n",
       " 101,\n",
       " 447,\n",
       " 77,\n",
       " 78,\n",
       " 77,\n",
       " 99,\n",
       " 138,\n",
       " 228,\n",
       " 76,\n",
       " 138,\n",
       " 73,\n",
       " 89,\n",
       " 68,\n",
       " 57,\n",
       " 61,\n",
       " 120,\n",
       " 72,\n",
       " 74,\n",
       " 85,\n",
       " 92,\n",
       " 168,\n",
       " 273,\n",
       " 67,\n",
       " 142,\n",
       " 163,\n",
       " 209,\n",
       " 33,\n",
       " 35,\n",
       " 201,\n",
       " 185,\n",
       " 61,\n",
       " 218,\n",
       " 81,\n",
       " 52,\n",
       " 120,\n",
       " 180,\n",
       " 52,\n",
       " 30,\n",
       " 99,\n",
       " 28,\n",
       " 147,\n",
       " 144,\n",
       " 102,\n",
       " 176,\n",
       " 478,\n",
       " 113,\n",
       " 217,\n",
       " 82,\n",
       " 71,\n",
       " 97,\n",
       " 79,\n",
       " 114,\n",
       " 42,\n",
       " 79,\n",
       " 52,\n",
       " 95,\n",
       " 71,\n",
       " 82,\n",
       " 102,\n",
       " 101,\n",
       " 58,\n",
       " 78,\n",
       " 63,\n",
       " 153,\n",
       " 54,\n",
       " 88,\n",
       " 132,\n",
       " 78,\n",
       " 82,\n",
       " 28,\n",
       " 76,\n",
       " 184,\n",
       " 532,\n",
       " 67,\n",
       " 115,\n",
       " 84,\n",
       " 155,\n",
       " 78,\n",
       " 437,\n",
       " 66,\n",
       " 74,\n",
       " 138,\n",
       " 249,\n",
       " 513,\n",
       " 90,\n",
       " 74,\n",
       " 82,\n",
       " 87,\n",
       " 83,\n",
       " 62,\n",
       " 86,\n",
       " 74,\n",
       " 53,\n",
       " 69,\n",
       " 45,\n",
       " 107,\n",
       " 77,\n",
       " 124,\n",
       " 120,\n",
       " 117,\n",
       " 100,\n",
       " 248,\n",
       " 96,\n",
       " 528,\n",
       " 76,\n",
       " 28,\n",
       " 88,\n",
       " 57,\n",
       " 336,\n",
       " 390,\n",
       " 48,\n",
       " 30,\n",
       " 285,\n",
       " 104,\n",
       " 68,\n",
       " 183,\n",
       " 84,\n",
       " 115,\n",
       " 189,\n",
       " 111,\n",
       " 64,\n",
       " 56,\n",
       " 254,\n",
       " 73,\n",
       " 43,\n",
       " 104,\n",
       " 67,\n",
       " 80,\n",
       " 79,\n",
       " 65,\n",
       " 96,\n",
       " 130,\n",
       " 128,\n",
       " 89,\n",
       " 163,\n",
       " 81,\n",
       " 59,\n",
       " 69,\n",
       " 118,\n",
       " 137,\n",
       " 253,\n",
       " 179,\n",
       " 245,\n",
       " 101,\n",
       " 111,\n",
       " 246,\n",
       " 92,\n",
       " 146,\n",
       " 82,\n",
       " 140,\n",
       " 294,\n",
       " 157,\n",
       " 120,\n",
       " 33,\n",
       " 74,\n",
       " 77,\n",
       " 195,\n",
       " 56,\n",
       " 120,\n",
       " 171,\n",
       " 58,\n",
       " 79,\n",
       " 288,\n",
       " 200,\n",
       " 55,\n",
       " 34,\n",
       " 204,\n",
       " 155,\n",
       " 38,\n",
       " 91,\n",
       " 77,\n",
       " 118,\n",
       " 35,\n",
       " 61,\n",
       " 350,\n",
       " 80,\n",
       " 179,\n",
       " 312,\n",
       " 72,\n",
       " 45,\n",
       " 72,\n",
       " 167,\n",
       " 166,\n",
       " 33,\n",
       " 259,\n",
       " 225,\n",
       " 153,\n",
       " 196,\n",
       " 97,\n",
       " 447,\n",
       " 92,\n",
       " 345,\n",
       " 81,\n",
       " 135,\n",
       " 126,\n",
       " 94,\n",
       " 453,\n",
       " 92,\n",
       " 92,\n",
       " 22,\n",
       " 289,\n",
       " 241,\n",
       " 272,\n",
       " 307,\n",
       " 229,\n",
       " 49,\n",
       " 103,\n",
       " 99,\n",
       " 208,\n",
       " 145,\n",
       " 77,\n",
       " 164,\n",
       " 68,\n",
       " 118,\n",
       " 93,\n",
       " 137,\n",
       " 88,\n",
       " 173,\n",
       " 117,\n",
       " 88,\n",
       " 138,\n",
       " 75,\n",
       " 80,\n",
       " 163,\n",
       " 156,\n",
       " 265,\n",
       " 138,\n",
       " 181,\n",
       " 117,\n",
       " 181,\n",
       " 86,\n",
       " 103,\n",
       " 53,\n",
       " 93,\n",
       " 48,\n",
       " 38,\n",
       " 122,\n",
       " 160,\n",
       " 127,\n",
       " 152,\n",
       " 95,\n",
       " 82,\n",
       " 121,\n",
       " 120,\n",
       " 112,\n",
       " 86,\n",
       " 270,\n",
       " 75,\n",
       " 209,\n",
       " 85,\n",
       " 38,\n",
       " 99,\n",
       " 71,\n",
       " 79,\n",
       " 48,\n",
       " 222,\n",
       " 26,\n",
       " 296,\n",
       " 91,\n",
       " 74,\n",
       " 52,\n",
       " 127,\n",
       " 84,\n",
       " 161,\n",
       " 60,\n",
       " 92,\n",
       " 214,\n",
       " 186,\n",
       " 138,\n",
       " 418,\n",
       " 73,\n",
       " 150,\n",
       " 63,\n",
       " 47,\n",
       " 75,\n",
       " 83,\n",
       " 67,\n",
       " 83,\n",
       " 72,\n",
       " 284,\n",
       " 32,\n",
       " 93,\n",
       " 113,\n",
       " 99,\n",
       " 71,\n",
       " 166,\n",
       " 87,\n",
       " 90,\n",
       " 132,\n",
       " 63,\n",
       " 82,\n",
       " 62,\n",
       " 62,\n",
       " 77,\n",
       " 120,\n",
       " 45,\n",
       " 64,\n",
       " 207,\n",
       " 224,\n",
       " 15,\n",
       " 166,\n",
       " 105,\n",
       " 145,\n",
       " 39,\n",
       " 67,\n",
       " 68,\n",
       " 344,\n",
       " 87,\n",
       " 80,\n",
       " 267,\n",
       " 97,\n",
       " 143,\n",
       " 63,\n",
       " 66,\n",
       " 97,\n",
       " 100,\n",
       " 85,\n",
       " 333,\n",
       " 162,\n",
       " 76,\n",
       " 71,\n",
       " 101,\n",
       " 265,\n",
       " 101,\n",
       " 66,\n",
       " 91,\n",
       " 180,\n",
       " 162,\n",
       " 115,\n",
       " 50,\n",
       " 92,\n",
       " 108,\n",
       " 86,\n",
       " 65,\n",
       " 43,\n",
       " 196,\n",
       " 87,\n",
       " 163,\n",
       " 221,\n",
       " 396,\n",
       " 217,\n",
       " 67,\n",
       " 70,\n",
       " 174,\n",
       " 72,\n",
       " 244,\n",
       " 97,\n",
       " 270,\n",
       " 99,\n",
       " 37,\n",
       " 86,\n",
       " 175,\n",
       " 503,\n",
       " 202,\n",
       " 58,\n",
       " 77,\n",
       " 48,\n",
       " 135,\n",
       " 97,\n",
       " 101,\n",
       " 62,\n",
       " 70,\n",
       " 122,\n",
       " 353,\n",
       " 64,\n",
       " 468,\n",
       " 335,\n",
       " 117,\n",
       " 66,\n",
       " 93,\n",
       " 299,\n",
       " 127,\n",
       " 17,\n",
       " 289,\n",
       " 101,\n",
       " 91,\n",
       " 524,\n",
       " 182,\n",
       " 32,\n",
       " 36,\n",
       " 68,\n",
       " 97,\n",
       " 60,\n",
       " 148,\n",
       " 293,\n",
       " 158,\n",
       " 84,\n",
       " 73,\n",
       " 152,\n",
       " 83,\n",
       " 192,\n",
       " 28,\n",
       " 206,\n",
       " 222,\n",
       " 74,\n",
       " 468,\n",
       " 162,\n",
       " 153,\n",
       " 191,\n",
       " 134,\n",
       " 319,\n",
       " 542,\n",
       " 177,\n",
       " 49,\n",
       " 303,\n",
       " 159,\n",
       " 76,\n",
       " 200,\n",
       " 167,\n",
       " 83,\n",
       " 84,\n",
       " 122,\n",
       " 173,\n",
       " 263,\n",
       " 74,\n",
       " 128,\n",
       " 129,\n",
       " 267,\n",
       " 178,\n",
       " 66,\n",
       " 71,\n",
       " 87,\n",
       " 84,\n",
       " 105,\n",
       " 106,\n",
       " 116,\n",
       " 131,\n",
       " 70,\n",
       " 213,\n",
       " 213,\n",
       " 173,\n",
       " 318,\n",
       " 86,\n",
       " 147,\n",
       " 237,\n",
       " 76,\n",
       " 48,\n",
       " 414,\n",
       " 68,\n",
       " 161,\n",
       " 82,\n",
       " 101,\n",
       " 105,\n",
       " 101,\n",
       " 140,\n",
       " 95,\n",
       " 44,\n",
       " 85,\n",
       " 74,\n",
       " 62,\n",
       " 218,\n",
       " 55,\n",
       " 103,\n",
       " 218,\n",
       " 68,\n",
       " 167,\n",
       " 289,\n",
       " 115,\n",
       " 102,\n",
       " 73,\n",
       " 172,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=int(np.ceil(np.mean(max_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[   49  5398  5873 ...     0     0     0]\n",
      " [    2  1042    24 ...     0     0     0]\n",
      " [16639   640  1817 ...  5956  1235     1]\n",
      " ...\n",
      " [   66  2975  2825 ...     0     0     0]\n",
      " [ 1155   395  3254 ...    14     6    23]\n",
      " [    8     3  2067 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  449  1536     3 ...     0     0     0]\n",
      " [66836 73462    91 ...   932   458    60]\n",
      " [    2 13835  1714 ...     0     0     0]\n",
      " ...\n",
      " [   54  1423    15 ...     0     0     0]\n",
      " [    1   293  2059 ...     0     0     0]\n",
      " [  789  1475   717 ... 13595  7040 14472]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "#Encode Review \n",
    "token = Tokenizer(lower=False)#lower=flase because we have already made every word lower case \n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Building the RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 130, 32)           2956160   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,981,057\n",
      "Trainable params: 2,981,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, it is simple. We only need to fit our x_train (input) and y_train (output/label) data. For this training, I use a mini-batch learning method with a batch_size of 128 and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9594\n",
      "Epoch 1: accuracy improved from -inf to 0.95938, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 137ms/step - loss: 0.1249 - accuracy: 0.9594\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9783\n",
      "Epoch 2: accuracy improved from 0.95938 to 0.97825, saving model to LSTM.h5\n",
      "313/313 [==============================] - 48s 152ms/step - loss: 0.0750 - accuracy: 0.9783\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9856\n",
      "Epoch 3: accuracy improved from 0.97825 to 0.98563, saving model to LSTM.h5\n",
      "313/313 [==============================] - 42s 135ms/step - loss: 0.0536 - accuracy: 0.9856\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9887\n",
      "Epoch 4: accuracy improved from 0.98563 to 0.98870, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 137ms/step - loss: 0.0460 - accuracy: 0.9887\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9911\n",
      "Epoch 5: accuracy improved from 0.98870 to 0.99112, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 137ms/step - loss: 0.0371 - accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9919\n",
      "Epoch 6: accuracy improved from 0.99112 to 0.99185, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 136ms/step - loss: 0.0331 - accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9934\n",
      "Epoch 7: accuracy improved from 0.99185 to 0.99345, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 138ms/step - loss: 0.0286 - accuracy: 0.9934\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9942\n",
      "Epoch 8: accuracy improved from 0.99345 to 0.99418, saving model to LSTM.h5\n",
      "313/313 [==============================] - 43s 136ms/step - loss: 0.0267 - accuracy: 0.9942\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9948\n",
      "Epoch 9: accuracy improved from 0.99418 to 0.99485, saving model to LSTM.h5\n",
      "313/313 [==============================] - 47s 150ms/step - loss: 0.0237 - accuracy: 0.9948\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9946\n",
      "Epoch 10: accuracy did not improve from 0.99485\n",
      "313/313 [==============================] - 46s 148ms/step - loss: 0.0230 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2752bc8bbe0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Testing\n",
    "To evaluate the model, we need to predict the sentiment using our x_test data and comparing the predictions with y_test (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of 86.63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Prediction: 8596\n",
      "Wrong Prediction: 1404\n",
      "Accuracy: 85.96000000000001\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = np.round(y_pred).astype(int)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review: A load of crap!! I am telling you now, please do not watch this film, it is a waste of money and a waste of time. Instead you could actually be having fun!\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  A load of crap I am telling you now please do not watch this film it is a waste of money and a waste of time Instead you could actually be having fun\n",
      "Filtered:  ['a load crap i telling please watch film waste money waste time instead could actually fun']\n"
     ]
    }
   ],
   "source": [
    "#Let's preprocess the input before entering it into the model \n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered_text = [w for w in words if w not in stopwords]\n",
    "filtered_text = ' '.join(filtered_text)\n",
    "filtered_text = [filtered_text.lower()]\n",
    "\n",
    "print('Filtered: ', filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = filtered_text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenize_words = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5, 6, 7, 8, 9, 1, 10, 1, 11, 12, 13, 14, 15]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words = pad_sequences(tokenize_words, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  3  4  5  6  7\n",
      "   8  9  1 10  1 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04770437]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.5:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
